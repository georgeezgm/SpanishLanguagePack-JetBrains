alibaba.bucket.create.dialog.hierarchical.field=계층형 네임스페이스
alibaba.bucket.create.dialog.versioning=버전 관리 활성화
alibaba.region.oss-ap-northeast-1=일본(도쿄)
alibaba.region.oss-ap-south-1=인도(뭄바이)
alibaba.region.oss-ap-southeast-1=호주(시드니) 1
alibaba.region.oss-ap-southeast-2=호주(시드니) 2
alibaba.region.oss-ap-southeast-3=말레이시아(쿠알라 룸푸르)
alibaba.region.oss-ap-southeast-5=인도네시아(자카르타)
alibaba.region.oss-ap-southeast-6=필리핀(마닐라)
alibaba.region.oss-cn-beijing=중국(베이징)
alibaba.region.oss-cn-chengdu=중국(청두)
alibaba.region.oss-cn-guangzhou=중국(광저우)
alibaba.region.oss-cn-hangzhou=중국(항저우)
alibaba.region.oss-cn-heyuan=중국(허위안)
alibaba.region.oss-cn-hongkong=중국(홍콩)
alibaba.region.oss-cn-huhehaote=중국(후허하오터)
alibaba.region.oss-cn-qingdao=중국(칭따오)
alibaba.region.oss-cn-shanghai=중국(상하이)
alibaba.region.oss-cn-shenzhen=중국(센첸)
alibaba.region.oss-cn-wulanchabu=중국(우란차부)
alibaba.region.oss-cn-zhangjiakou=중국(장자커우)
alibaba.region.oss-eu-central-1=독일(프랑크푸르트)
alibaba.region.oss-eu-west-1=영국(런던)
alibaba.region.oss-me-east-1=UAE(두바이)
alibaba.region.oss-us-east-1=미국(버지니아)
alibaba.region.oss-us-west-1=미국(실리콘 밸리)
alibaba.settings.credentials.file=Alibaba 자격 증명 파일
alibaba.task.delete.bucket.text=버킷 {0} 삭제 중
alibaba.task.delete.directory.text=디렉터리 {0} 삭제 중
alibaba.task.delete.directory.text2=이미 삭제된 {0}개의 객체
alibaba.task.delete.file.text=파일 {0} 삭제 중
azure.column.name.access.tier=액세스 티어
azure.rename.text2.indicator.deleting={0}: {1} 삭제 중
bucket.name.is.empty.for.path=경로 ''{0}''의 버킷 이름이 비어 있습니다
cannot.find.linode.region={0}의 리전을 찾을 수 없습니다
cannot.open.read.stream.null.blob=null blob {0}의 읽기 스트림을 열 수 없습니다
client.is.not.inited=클라이언트가 초기화되지 않았습니다
connection.error.fs.and.user.not.found=URI {0} 및 사용자 {1}의 파일 시스템을 찾을 수 없습니다
connection.error.hadoop.home.is.not.defined.full=HADOOP_HOME이 정의되지 않았습니다. Windows에서 HADOOP_HOME 환경 변수를 정의하거나 Java 프로퍼티 hadoop.home.dir이 필요합니다. 자세한 내용은 <a href=\"https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\">Hadoop Wiki</a>를 참조하세요.
connection.error.hadoop.no.native.drivers.full=HADOOP_HOME에서 네이티브 드라이버를 찾을 수 없습니다. 자세한 내용은 <a href=\"https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\">Hadoop Wiki</a>를 참조하세요.
connection.error.root.path.must.be.non.empty=루트 경로는 비어 있을 수 없습니다
controller.cluster.instances.error=클러스터 인스턴스 업데이트 오류
controller.cluster.steps.error=클러스터 스텝 업데이트 오류
copy.failed={0}에서 {1}(으)로 복사하지 못했습니다.
custom.bucket.text.empty=bucket/folder,bucket2/folder/subfolder2,...
custom.bucket.text.hint=소스 루트 목록을 지정하려면 ',' 구분 기호를 사용합니다(bucket1/folder1/folder2,bucket2/folder)
do.region.ams3=네덜란드 암스테르담
do.region.fra1=독일 프랑크푸르트
do.region.nyc3=미국 뉴욕
do.region.sfo=미국 샌프란시스코
do.region.sgp1=싱가포르
emr.cluster.filter=상태별 필터링
emr.cluster.filter.limit=제한
emr.cluster.info.details=JSON으로 표시
emr.cluster.terminate.cluster.message=클러스터 ''{0}''을(를) 종료하시겠어요?
emr.cluster.terminate.cluster.title=클러스터 종료 중
emr.connection.creation=EMR 연결 생성
emr.connection.warning.no.clusters=연결되었습니다. 클러스터가 없습니다
emr.connection.warning.no.clusters.desc=연결되었으나 선택된 리전에서 클러스터를 찾을 수 없습니다. 리전이 올바른지 확인하세요.
emr.connection.warning.no.clusters.desc.window={0} 리전에서 클러스터를 찾을 수 없습니다. 리전을 확인하세요.
emr.dialog.title.select.key.info.cancel=취소
emr.dialog.title.select.key.info.msg=연결하려면 SSH 터널을 생성해야 합니다. 설정하려면 클러스터의 SSH 키 파일을 선택하세요.
emr.dialog.title.select.key.info.ok=SSH 키 선택
emr.dialog.title.select.key.info.title=SSH 키 필요
emr.dialog.title.select.key.ssh.file=키 SSH 파일 선택
emr.error=AWS EMR 예외
emr.error.remove.cluster=클러스터 오류 제거
emr.error.start.cluster=클러스터 오류 시작
emr.error.stop.cluster=클러스터 오류 중지
emr.filter.text=필터:
emr.is.not.inited=EMR 클라이언트가 초기화되지 않았습니다
emr.key.storage.dialog.title=EMR SSH 키스토어
emr.keys.settings.column.key.name=키 이름
emr.keys.settings.column.key.path=경로
emr.keys.settings.label=SSH 키:
emr.keys.settings.table.empty=SSH 키가 제공되지 않았습니다
emr.label.choose.key.file.for.aws.pair=AWS {0} 쌍의 키 파일 선택
emr.remove.linked.connections.action=연결 제거
emr.remove.linked.connections.desc=EMR용으로 생성된 연결을 삭제하시겠어요?
emr.remove.linked.connections.title=EMR 연결
emr.spark.submit=EMR Spark-submit
emr.spark.submit.editor.args=인수:
emr.spark.submit.editor.jar.loc=JAR 위치:
emr.spark.submit.editor.name=이름:
emr.step.details=단계 세부 정보 표시
emr.step.mapper.choose=매퍼 선택
emr.step.reducer.choose=리듀서 선택
emr.step.s3.input.choose=S3 입력 선택
emr.step.s3.output.choose=S3 출력 선택
emr.step.script.choose=스크립트 S3 위치 선택
emr.toolwindow.title=AWS EMR
error.krb5.conf=Kerberos로 승인할 수 없습니다. "allow_weak_crypto = true"를 krb5.conf에 추가해 보세요
error.object.summary.is.not.found={0}의 객체 요약이 없습니다
file.info.access.blob.type=blob 타입
file.info.access.content.type=콘텐츠 타입
file.info.access.tier=액세스 티어
file.info.access.tier.modified=액세스 티어 마지막 수정일
gcs.buckets.source=버킷 소스:
gcs.connection.browse.title=자격 증명 JSON 선택
gcs.connection.error.bucket.validation1=버킷 이름은 3~63자여야 합니다. 마침표를 포함하여 최대 222자까지 포함할 수 있으나 마침표로 구분된 각 구성 요소는 63자를 넘을 수 없습니다.
gcs.connection.error.bucket.validation2=이름에는 소문자, 숫자, 대시(-), 밑줄(_) 및 마침표(.)만 포함할 수 있습니다
gcs.connection.error.bucket.validation3=버킷 이름은 숫자 또는 문자로 시작하고 끝나야 합니다.
gcs.connection.error.bucket.validation4=버킷 이름이 마침표로 구분된 10진수 형태로 표시된 IP 주소일 수 없습니다(예: 192.168.5.4).
gcs.connection.error.bucket.validation5=버킷 이름은 'goog' 접두사로 시작할 수 없습니다.
gcs.connection.error.bucket.validation6=버킷 이름은 'google'을 포함하거나 'g00gle'처럼 비슷한 오기를 포함할 수 없습니다.
gcs.connection.error.cred.file.not.selected=계정의 자격 증명 파일이 선택되어야 합니다
gcs.connection.error.file.not.exists=파일이 없습니다
gcs.custom.url=사용자 지정 호스트:
gcs.json.location.emptyText=클라우드 스토어 JSON 위치
gcs.multibucket.update.text=Google Cloud 스토리지에서 다중 버킷이 지원됩니다. 버킷은 연결 설정에서 구성할 수 있습니다.
gcs.multibucket.update.title=BigDataTools의 새로운 GCS 기능
gcs.progress.details.deleting={0} 삭제 중
gcs.project.id=프로젝트 ID:
gcs.project.id.emptyText=선택적 덮어쓰기 프로젝트 ID
gcs.project.id.hint=특별 프로젝트 ID의 버킷 표시
gcs.public.hint=공개 버킷에서는 비워놓기
gcs.sdk.install=Google Cloud SDK를 찾을 수 없습니다. <a>설치</a>
gcs.sdk.update=Google Cloud SDK가 오래되었습니다. <a>업데이트</a>
group.name.alibaba=Alibaba OSS
group.name.azure=Azure
group.name.dospaces=DigitalOcean Spaces
group.name.emr=AWS EMR
group.name.gcs=Google Cloud 스토리지
group.name.hdfs.java=HDFS
group.name.linode=Linode
group.name.minio=MinIO
group.name.s3=AWS S3
group.name.yandex=Yandex 객체 스토리지
group.names.hdfs.data=HDFS 문제
hdfs.column.name.access.time=액세스 일시
hdfs.column.name.block.size=블록 크기
hdfs.column.name.group=그룹화
hdfs.column.name.is.encrypted=암호화됨
hdfs.column.name.is.isErasureCoded=이레이저로 코딩됨
hdfs.column.name.is.isSnapshotEnabled=스냅샷
hdfs.column.name.owner=소유자
hdfs.column.name.permission=권한
hdfs.column.name.replications=복제
hdfs.config.path.does.not.exist=지정된 디렉터리가 없습니다
hdfs.config.path.no.xmls.found=지정된 디렉터리에 XML 파일이 없습니다
hdfs.config.path.not.empty=구성 경로는 공백일 수 없습니다
hdfs.config.path.should.be.directory=구성 경로는 디렉터리를 가리켜야 합니다
hdfs.config.path.title=Java API 구성 경로
hdfs.field.root.path=루트 경로
hdfs.file.info.label.accessTime=액세스 시간:
hdfs.file.info.label.block.size=블록 크기:
hdfs.file.info.label.group=그룹화:
hdfs.file.info.label.isEncrypted=암호화됨:
hdfs.file.info.label.isErasureCoded=이레이저로 코딩됨:
hdfs.file.info.label.isSnapshotEnabled=스냅샷 활성화됨:
hdfs.file.info.label.modificationTime=수정된 시간:
hdfs.file.info.label.owner=소유자:
hdfs.file.info.label.permission=권한:
hdfs.file.info.label.replication=복제:
hdfs.file.info.label.size=크기:
hdfs.is.not.inited=Hdfs 연결이 초기화되지 않았습니다
hdfs.java.config.source=구성 소스:
hdfs.java.driver.home.path=드라이버 홈 경로:
hdfs.no.xmls.in.directory=구성 루트에 XML 파일이 없습니다
hdfs.property.source.directory=구성 폴더
hdfs.property.source.explicit=사용자 지정
hdfs.root.folder.does.not.exist=루트 폴더 ''{0}''이(가) 없습니다
hdfs.ssh.tunnel.ssh.operation.not.supported=SSH 터널을 통한 작업은 지원되지 않습니다.
inspection.java.custom.hdfs.format.display.name=사용자 지정 HDFS 파일 서식 강조 표시
inspection.java.invalid.file.path.display.name=잘못된 HDFS 파일 경로 강조 표시
inspection.kotlin.custom.hdfs.format.display.name=사용자 지정 HDFS 파일 서식 강조 표시
inspection.kt.invalid.file.path.display.name=잘못된 HDFS 파일 경로 강조 표시
inspection.non.serializable.data.in.scope.display.name=비 Serializable 데이터를 Spark 작업에서 강조 표시
inspection.scala.custom.hdfs.format.display.name=사용자 지정 HDFS 파일 서식 강조 표시
inspection.scala.invalid.hdfs.file.path.display.name=잘못된 HDFS 파일 경로 강조 표시
invalid.format.inspection.description=사용자 지정 hdfs 형식을 강조 표시. 기본적으로 parquet, orc, sequence, json, csv 또는 텍스트가 필요합니다.
invalid.format.inspection.template=예기치 않은 사용자 지정 파일 형식
java.wrong.path.inspection.description=Java 코드 내 올바르지 않은 hdfs 경로를 강조 표시
kerberos.type.credentials=비밀번호
kerberos.type.disabled=비활성화됨
kerberos.type.keytab=Keytab
kerberos.type.subject=JAAS 구성(고급)
kotlin.wrong.path.inspection.description=Kotlin 코드 내 올바르지 않은 hdfs 경로를 강조 표시
linode.region.ap-south=AP 남부(싱가포르)
linode.region.eu-central=EU 중부(독일 프랑크푸르트)
linode.region.us-east=미국 동부(미국 뉴워크)
linode.region.us-southeast=미국 동남부(미국 애틀랜타)
metainfo.headers.empty=사용자 지정 헤더가 없습니다
metainfo.headers.key=키
metainfo.headers.value=값
metainfo.section.custom.headers=헤더
minio.region.text.empty=디폴트 값 사용
move.failed={0}에서 {1}(으)로 이동하지 못했습니다.
notification.group.orc.files=ORC 파일
oss.file.info.label.hns.status=계층형 네임스페이스:
rfs.create.bucket.message=버킷 생성
s3.bucket.text.empty=모든 버킷이 표시됩니다
s3.bucket.text.hint=이 필드가 비어 있으면 모든 버킷이 표시됩니다<br>버킷 이름을 입력하고 필터 종류를 '일치'로 선택하면 하나의 버킷으로 작업할 수 있습니다<br>버킷을 구분하려면 ','를 사용합니다(bucket1,bucket2)
s3.column.name.etag=ETag
s3.column.name.metadata=메타데이터
s3.column.name.storage.class=스토리지 클래스
s3.connection.error.ssh.without.endpoint=SSH 터널을 사용하려면 드라이버의 엔드포인트를 지정하세요
s3.empty.directories.not.allowed=빈 디렉터리는 생성할 수 없습니다
s3.multibucket.open.settings=설정 열기
s3.multibucket.update.text=S3 호환 스토리지에서 다중 버킷이 지원됩니다. 버킷은 연결 설정에서 구성할 수 있습니다.
s3.multibucket.update.title=BigDataTools의 새로운 S3 기능
s3.operations.timeout.hint=원격 서버의 작업에 대한 시간 초과(초)
scala.serializable.scope.inspection.description=spark 작업 범위에서 런타임 예외가 발생할 수 있는 직렬화가 불가능한 값을 강조 표시합니다.
scala.serializable.scope.inspection.warning=<html>spark 범위 {2} 내 타입 {1}의 직렬화 불가능한 값 {0}</html>
scala.wrong.path.inspection.description=Scala 코드 내 올바르지 않은 hdfs 경로를 강조 표시
settings.alibaba.region=리전:
settings.azure.auth.type=인증 타입:
settings.azure.connection.string=연결 문자열:
settings.azure.container=컨테이너:
settings.azure.endpoint=엔드포인트:
settings.azure.password=비밀번호:
settings.azure.sas.token=SAS 토큰:
settings.azure.user.key=키:
settings.azure.username=사용자 이름:
settings.bucket.filter=버킷 필터:
settings.bucket.filter.type=필터 타입:
settings.buckets.custom.list=사용자 지정 루트
settings.buckets.hint=<html><b>계정 내의 모든 버킷</b> - 일종의 <it>list buckets</it> 요청을 실행합니다. 이 결과로 생기는 버킷 목록은 필터링할 수 있습니다.<br><br><b>사용자 지정 루트</b> - 선택된 루트를 직접 요청합니다. 버킷뿐만 아니라 디렉터리의 전체 경로를 지정할 수 있습니다.</html>
settings.buckets.user.list=계정 내의 모든 버킷
settings.config.from.folder=분석된 구성:
settings.config.path=구성 경로:
settings.custom.roots=루트:
settings.generate.kerberos=Kerberos
settings.hdfs.auth.type=인증:
settings.hdfs.kerberos.type=인증 방법:
settings.hdfs.kinit=kinit 캐시 사용
settings.hdfs.url=클러스터 URI:
settings.hdfs.username=Hadoop 사용자 이름:
settings.hdfs.username.hint=서버 로그인에 필요한 사용자 이름입니다. 지정되지 않은 경우 <i>HAD00P_USER_NAME</i> 환경 변수가 사용됩니다. 이 변수가 정의되지 않은 경우 <i>user.name</i> 프로퍼티가 사용됩니다. Kerberos가 활성화된 경우 이 세 가지 값을 모두 재정의합니다.
settings.kerberos.auth=인증:
settings.kerberos.auth.kerberos=Kerberos
settings.kerberos.auth.none=없음
settings.minio.endpoint=엔드포인트:
settings.properties=고급 구성:
settings.s3.bucket.filter.by.region=선택된 리전의 버킷만
settings.s3.custom.endpoint=엔드포인트:
settings.s3.custom.region=리전:
settings.s3.custom.region.hint=필요 시 사용
settings.s3.region=리전:
settings.s3.region.group=AWS S3
settings.s3.selection.endpoint=S3 호환 스토리지
settings.undefined.path=<초기화되지 않음>
settings.validation.kerberos.keytab.error=Keytab 및 규칙을 지정해야 합니다
settings.validation.kerberos.password.error=규칙 및 비밀번호를 지정해야 합니다
setup.video.tutor=연결 설정 튜토리얼 영상
ssh.additional.info=SSH 터널링은 <b>네임 노드가 있는 작업에만</b>(파일 나열, 메타 정보 얻기) 사용할 수 있습니다.<br><br>
ssh.additional.label=(NameNode 작업만)
wrong.region=리전 {0}을(를) 찾을 수 없습니다