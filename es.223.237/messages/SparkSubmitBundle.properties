# Common ui
configuration.name=Spark 제출
configuration.name.local=로컬
configuration.description=Spark Submit configuration
configuration.name.ssh=SSH

# Label "Add options" and popup menu
configuration.options.add=Add options
configuration.options.add.title=Add Submit Options
# General settings block
settings.spark.home=Spark home:
settings.spark.python.sdk=Run with a Python env:
settings.ssh.config=SSH 구성:
settings.ssh.target.dir=Target upload directory:
settings.ssh.target.dir.hint=<html>Server folder where all <i>UPLOAD</i> files will be uploaded before execution of spark-submit command.<br>Please note that this file can be overwritten after execution.</html>
settings.url.upload.name=Upload File
settings.url.server.name=Server File
settings.url.server.mock.desc=파일 경로:
settings.url.artifact.name=IDEA Artifact
settings.url.file.name=파일
settings.url.local.name=로컬
settings.url.s3.name=S3
settings.url.web.name=원격
settings.url.hdfs.name=HDFS
settings.url.custom.name=사용자 지정
settings.deploymode.client=클라이언트
settings.deploymode.cluster=클러스터
settings.application=응용프로그램:

settings.application.class.name=클래스:
settings.application.class.hint=--class Your application's main class (for Java / Scala apps).


settings.application.arguments=Run arguments:
settings.application.hint=Arguments passed to the main method of your main class, if any.

settings.cluster.manager=Cluster manager:

settings.deploy.mode=Deploy Mode:
settings.deploy.mode.hint=<html>--deploy-mode Whether to launch the driver program locally ("client")<br>or on one of the worker machines inside the cluster ("cluster").</html>

settings.master=Master:
settings.master.hint=--master spark://host:port, mesos://host:port, yarn, k8s: //https://host:port, or local (Default: local[*]).

# Shell options
settings.shell.title=Shell Options
settings.isInteractive=대화형
settings.isInteractive.hint=Run execution command in shell interactive mode
settings.shellExecutor=셸 경로
settings.shellExecutor.hint=Path to shell. Used if interactive mode is enabled or before the submit script is set
settings.beforeShellScript=Before submit script
settings.workingDirectory=작업 디렉터리
settings.beforeShellScript.hint=Script which be executed before spark submit. For instance, "source activate py36"
settings.envParams=환경 변수
settings.envParams.hint=Additional environment variables

# ClusterManagerType
cluster.manager.local=로컬
cluster.manager.standalone=Standalone
cluster.manager.mesos=Apache Mesos
cluster.manager.yarn=Hadoop YARN
cluster.manager.kubernetes=Kubernetes
cluster.manager.nomad=Nomad
# Additional
settings.ssh.title=SFTP Options

settings.additional.title=Advanced Submit Options
settings.additional.verbose=Print additional debug output
# Cluster Manager
settings.cluster.manager.proxy.user=Proxy user:
settings.cluster.manager.proxy.user.hint=<html>--proxy-user User to impersonate, when submitting the application.<br>This argument has no effect with --principal/--keytab.</html>
settings.cluster.manager.queue=큐:
settings.cluster.manager.queue.hint=--queue The YARN queue to submit to (Default: "default").
settings.cluster.manager.supervise=Enable supervise
settings.cluster.manager.supervise.hint=--supervise If given, restarts the driver on its failure.
# Spark config
settings.spark.title=Spark Configuration
settings.spark.config=Configs:
settings.spark.config.hint=--conf Spark configuration properties.
settings.spark.properties.file=Properties File:
settings.spark.properties.file.hint=<html>--properties-file Path to a file from which to load extra properties.<br>If it is not specified, this will look for conf/spark-defaults.conf.</html>
# Dependencies
settings.dependencies.title=종속 요소

settings.dependencies.jars=Jars:
settings.dependencies.jars.hint=--jars Comma-separated list of jars to include on the driver and executor classpaths.

settings.dependencies.python=Py files:
settings.dependencies.python.hint=--py-files Comma-separated list of .zip, .egg, or .py files to add to the PYTHONPATH for Python apps.

settings.dependencies.files=파일:
settings.dependencies.files.hint=--files Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName).

# Executor
settings.executor.title=Executor
settings.executor.memory=Executor memory:
settings.executor.memory.hint=--executor-memory Memory per executor (for example, 1000M, 2G)
settings.executor.memory.default=1G
settings.executor.number=Executors number:
settings.executor.number.hint=<html>--num-executors Number of executors to launch<br>If dynamic allocation is enabled, the initial number of executors will be at least value.</html>
settings.executor.cores=Executor cores:
settings.executor.cores.hint=<html>--executor-cores Number of cores per executor<br>(Default: 1 in YARN mode, or all available cores on the worker in standalone mode).</html>
settings.executor.cores.total=Total executor cores:
settings.executor.cores.total.hint=--total-executor-cores Total cores for all executors.
settings.executor.archives=Archives:
settings.executor.archives.hint=<html>--archives List of archives to be extracted into the working directory of each executor.</html>
# Integration
settings.integration.title=Spark Monitoring Integration
settings.integration.spark.monitoring=연결:
settings.integration.spark.monitoring.add=새 항목 추가
# Kerberos
settings.kerberos.title=Kerberos

settings.kerberos.principal=규칙:
settings.kerberos.principal.hint=--principal Principal to be used to login to KDC, while running on secure HDFS.

settings.kerberos.keytab=Keytab:
settings.kerberos.keytab.hint=<html>--keytab The full path to the file that contains the keytab for the principal specified above<br>This keytab will be copied to the node running the Application Master via the Secure Distributed Cache,<br>for renewing the login tickets and the delegation tokens periodically.</html>
# Driver
settings.driver.title=드라이버

settings.driver.memory=Driver memory:
settings.driver.memory.hint=--driver-memory Memory for driver (e.g. 1000M, 2G).

settings.driver.cores=Driver cores:
settings.driver.cores.hint=--driver-cores Number of cores used by the driver, only in cluster mode.

settings.driver.java.options=Driver Java options:
settings.driver.java.options.hint=--driver-java-options Extra Java options to pass to the driver.

settings.driver.library.path=Driver library path:
settings.driver.library.path.hint=--driver-library-path Extra library path entries to pass to the driver.

settings.driver.class.path=Driver class path:
settings.driver.class.path.hint=<html>--driver-class-path Extra class path entries to pass to the driver<br>Note that jars added with --jars are automatically included in the classpath.</html>
# Maven
settings.maven.title=Maven

settings.maven.packages=패키지:
settings.maven.packages.hint=<html>--packages List of maven coordinates of jars to include on the driver and executor classpaths<br>Will search the local maven repo, then maven central and any additional remote repositories given by --repositories<br>The format for the coordinates should be groupId:artifactId:version.</html>

settings.maven.exclude.packages=Exclude packages:
settings.maven.exclude.packages.hint=<html>--exclude-packages List of groupId:artifactId<br>to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts.</html>

settings.maven.repositories=저장소:
settings.maven.repositories.hint=--repositories List of additional remote repositories to search for the maven coordinates given with --packages.

settings.ssh.error.title=File Chooser Error
settings.ssh.error.msg=Please, select an SSH configuration first
settings.application.class.name.error.msg=Please, specify an application first

# Select class name dialog
dialog.select.class.title=클래스 이름
dialog.select.class.empty=No classes found in the selected application

# Select artifact dialog
dialog.select.artifact.title=Select Dependency Artifact
dialog.select.artifact.empty=No artifacts in the current project
dialog.select.artifact.link.open.artifact.settings=Set up an artifact
dialog.select.artifact.button.open.artifact.settings=Artifacts settings

# Validation messages
error.ssh.config=SSH configuration should be specified

#Files upload
upload.files.error=Exception when uploading files. {0}
upload.files.success=Successfully uploaded {0} files
upload.target.dir.is.not.found=Target directory "{0}" is not found
upload.file.title=Upload files to remote

# Select file dialog title
dialog.targetDirectory.title=대상 디렉터리 선택
dialog.sparkHomePath.title=Select Spark Home Directory
dialog.workDir.title=작업 디렉터리 선택
dialog.artifactPath.title=Select Application
dialog.keytabFile.title=Select Keytab File
dialog.propertiesFile.title=Select Properties File
dialog.driverLibraryPath.title=Select Driver Library Path
dialog.driverClassPath.title=Select Driver Class Path
dialog.archives.title=Select Archive File
dialog.jars.title=Select Jar Files
dialog.files.title=파일 선택
dialog.pyfiles.title=Select Python Files
dialog.message.failed.to.create.ssh.process=SSH 프로세스를 생성하지 못했습니다
upload.files.through.sftp.to.spark.host=Upload Files Through SFTP

# Actions
action.open.local.run.config=Local Spark Submit
action.open.ssh.run.config=SSH Spark Submit
dialog.message.spark.home.should.be.set.to.correct.folder=$SPARK_HOME should be set to the correct folder.
receive.artifact.task=Receive Artifact...
fun.search.process.text=Process {0}
progress.text.upload.to.host=Upload {0} to the host...
row.final.command=Result Submit Command
row.final.command.hint=You can copy or load a spark-submit command and it will fill in the corresponding fields
row.final.command.copy=Copy spark-submit command
sparkhome.tooltip=Spark directory
work.directory.tooltip=Points to the script call location
artifact.tooltip=Path to an executable file to run on cluster

cluster.manager.tooltip=Cluster Manager configured on the server.
dialog.message.specify.application=Specify an application
dialog.message.not.found={0}을(를) 찾을 수 없습니다
load.command.string=Load a spark-submit command
dialog.input.spark.command.label=Spark command
dialog.input.spark.command.title=Spark Input
dialog.input.spark.command.description=Paste a spark-submit command here to fill the Spark Submit form
open.cluster.info.description=문서 열기