action.add.job.title=Submit Job
action.cancel.job.confirm.msg=''{0}'' 잡을 취소하시겠어요?
action.cancel.job.title=잡 취소
action.clone.job.title=잡 복제
action.cluster.remove.confirm.msg=클러스터 ''{0}''을(를) 삭제하시겠어요?
action.cluster.start.confirm.msg=클러스터 ''{0}''을(를) 시작하시겠어요?
action.cluster.terminate.confirm.msg=클러스터 ''{0}''을(를) 종료하시겠어요?
action.confirm.title=확인
action.delete.job.confirm.msg=''{0}'' 잡을 삭제하시겠어요?
action.delete.job.title=잡 삭제
action.open.stage.bucket=스테이지 버킷 열기
action.sftp=노드 연결 SFTP 열기
action.sftp.master.node=마스터 노드 연결 SFTP 열기
action.ssh=노드 연결 SSH 열기
action.ssh.master.node=SSH를 통해 마스터 노드에 연결
add.job.title=Submit Job
add.new.submit.connection.label=Dataproc 연결 추가...
cell.execution.finished.msg=잡 "{0}"이(가) {1} 상태로 완료되었습니다
cell.execution.finished.title=Dataproc 잡
cluster.action.delete=클러스터 삭제
cluster.action.start=클러스터 시작
cluster.action.stop=Terminate Cluster...
cluster.info.config.autoscaling=Autoscaling:
cluster.info.config.master.node.desc=Master node:
cluster.info.config.metastore=Dataproc Metastore:
cluster.info.config.monitoring=Integrity Monitoring:
cluster.info.config.network=네트워크:
cluster.info.config.region=영역:
cluster.info.config.scheduled.deletion=Scheduled deletion:
cluster.info.config.secure.boot=Secure boot:
cluster.info.config.vtpm=VTPM:
cluster.info.config.worker.node.desc=Worker nodes:
cluster.info.config.zone=Zone
cluster.info.image.created=Created:
cluster.info.image.version=Image version:
cluster.info.internal.ip=Internal IP only:
cluster.info.optional.components=Optional components:
cluster.info.summary.name=이름:
cluster.info.summary.state=상태:
cluster.info.summary.state.details=State details:
cluster.info.summary.type=Tipo:
cluster.info.summary.uiid=Cluster UUID:
cluster.tab.applications.title=애플리케이션
cluster.tab.info.title=정보
cluster.tab.jobs.title=잡(Job)
cluster.tab.name=클러스터
cluster.tab.vb.instances.title=VM Instances
data.clusterInfo.created=Created
data.clusterInfo.id=ID
data.clusterInfo.name=이름
data.clusterInfo.region=영역
data.clusterInfo.scheduledDeletion=예약된 삭제
data.clusterInfo.stagingBucket=스테이징 버킷
data.clusterInfo.state=상태
data.clusterInfo.totalWorkers=총 워커
data.clusterInfo.zone=지역
data.jobInfo.cluster=클러스터
data.jobInfo.elapsedTime=경과 시간
data.jobInfo.id=ID
data.jobInfo.labels=라벨
data.jobInfo.startTime=시작 시간
data.jobInfo.status=상태
data.jobInfo.type=타입
data.vm.instanceInfo.componentGateway=구성 요소 게이트웨이
data.vm.instanceInfo.name=이름
data.vm.instanceInfo.url=URL
data.web.interfaceInfo.name=이름
data.web.interfaceInfo.role=역할
datamanager.configuration=구성
datamanager.job.info=잡 정보
datamanager.labels=라벨
datamanager.properties=프로퍼티
datamanager.summary=요약
dataproc.error=Dataproc 오류
dataproc.error.cluster.must.be.started=클러스터를 실행해야 합니다.
dataproc.toolwindow.title=GC Dataproc
default.gcs.connection.name=GC Dataproc project
emr.remove.linked.connections.title=Dataproc connections
error.connection.is.not.found=Dataproc의 연결이 설정되지 않았습니다. 다시 생성하세요.
error.json.auth.limited.msg=이 연산은 gcloud CLI를 사용하여 Dataproc 내에서 인증할 때만 사용할 수 있습니다
error.json.auth.limited.title=연산을 사용할 수 없음
error.spark.is.not.found=클러스터에 Spark 기록 서버가 없습니다
exportable.DataprocSettings.presentable.name=Big Data Tools Dataproc 설정
exportable.DataprocSshKeyPaths.presentable.name=Big Data Tools Dataproc SSH 설정
group.name.dataproc=GC Dataproc
info.value.off=끄기
instance.config.gpu.number=Number of GPUs
instance.config.local.ssd=Local SSDs
instance.config.machineType=Machine type:
instance.config.primary.disk.size=Primary disk size:
instance.config.primary.disk.type=Primary disk type:
job.hadoop.title=Hadoop
job.hive.title=Hive
job.info.client.tags=Client tags
job.info.cluster=클러스터:
job.info.continue.on.failure=Continue on failure
job.info.elapsed.time=Elapsed time:
job.info.jobId=Job ID:
job.info.jobUuid=Job UUID:
job.info.max.restart.per.hour=Max restart per hour
job.info.max.restart.per.hour.hint=Leave blank if you don't want to allow automatic restarts on job failure.
job.info.open.job.files=GCS 내의 잡 폴더 표시
job.info.properties=프로퍼티
job.info.query.file=쿼리:
job.info.query.file.value=Query file:
job.info.query.text.value=Query text:
job.info.query.type=Query source:
job.info.single.file.hint=Can be a GCS file with the gs:// prefix, an HDFS file on the cluster with the hdfs:// prefix, or a local file on the cluster with the file:// prefix
job.info.spark.additional.py.files=Additional python files:
job.info.spark.additional.py.files.title=Select Additional Py File
job.info.spark.additional.r.files=Addtional R files:
job.info.spark.additional.r.files.title=Select Additional R File
job.info.spark.archives=Archives:
job.info.spark.archives.hint=Archive files are extracted in the Spark working directory. Can be a GCS file with the gs:// prefix, an HDFS file on the cluster with the hdfs:// prefix, or a local file on the cluster with the file:// prefix. Supported file types: .jar, .tar, .tar.gz, .tgz, .zip.
job.info.spark.archives.title=Select Archive
job.info.spark.args=인수:
job.info.spark.files=Archivo:
job.info.spark.jars=Jars:
job.info.spark.jars.hint=Jar files are included in the CLASSPATH. Can be a GCS file with the gs:// prefix, an HDFS file on the cluster with the hdfs:// prefix, or a local file on the cluster with the file:// prefix.
job.info.spark.jars.title=JARs
job.info.spark.main.class=메인 클래스:
job.info.spark.main.py.file.title=Select Main Py File
job.info.spark.main.pyfile=Main python file:
job.info.spark.main.r.file=Main R file:
job.info.spark.main.r.file.title=Select Main R File
job.info.start.date=Start Date:
job.info.status=상태:
job.info.status.details=Status details:
job.info.type=Job type:
job.label.block.title=라벨
job.pig.title=Pig
job.presto.title=Presto
job.properties.block.title=프로퍼티
job.pyspark.title=PySpark
job.query.file.dialog.title=Select Query File:
job.query.file.label=Query file:
job.query.source.file=파일
job.query.source.text=Texto
job.query.source.type=Query type:
job.query.text.hint=The query to execute
job.query.text.label=Query text:
job.spark.r.title=SparkR
job.spark.sql.title=SparkSql
job.spark.title=Spark
job.state.active=활성
job.state.canceled=취소됨
job.state.done=완료
job.state.failed=실패
job.validation.file.archive={0} must be archive type .jar, .tar, .tar.gz, .tgz, .zip.
job.validation.file.fs={0} must be file with the gs://, hdfs:// or file:// prefix
metainfo.cluster.id=ID:
metainfo.cluster.name=이름:
metainfo.cluster.status=상태:
remote.target.emr.cluster.remark=Dataproc
resolve.artifact.is.not.supported={0}의 메인 클래스 탐지는 지원되지 않습니다.
settings.application.class.name.error.msg=jar 파일을 먼저 선택하세요
task.init.ssh.perform.cli.command=GCloud CLI 명령어 실행 중...
task.init.ssh.title=Dataproc CLI 실행
