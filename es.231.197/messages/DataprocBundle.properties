
cluster.action.stop=Terminate Cluster...
cluster.details.not.found=Info for cluster {0} is not found. Please refresh EMR cluster list.
cluster.state.terminated.last.week=Terminated last week
cluster.state.terminated.last.day=Terminated last day
cluster.state.terminated.last.hour=Terminated last hour
cluster.state.terminated=종료됨
cluster.state.running=활성
cluster.state.starting=시작 중
metainfo.cluster.name=이름:
metainfo.cluster.id=ID:
metainfo.cluster.status=상태:
cluster.state.failed=Fallido
group.name.dataproc=GC Dataproc
emr.remove.linked.connections.title=Dataproc connections
dataproc.toolwindow.title=GC Dataproc
cluster.info.summary.name=이름:
cluster.info.summary.uiid=Cluster UUID:
cluster.info.summary.type=Tipo:
cluster.info.summary.state=상태:
cluster.info.summary.state.details=State details:
cluster.info.config.region=영역:
cluster.info.config.zone=Zone
cluster.info.config.autoscaling=Autoscaling:
cluster.info.config.metastore=Dataproc Metastore:
cluster.info.config.scheduled.deletion=Scheduled deletion:
info.value.off=끄기
cluster.info.config.master.node.desc=Master node:
instance.config.machineType=Machine type:
instance.config.gpu.number=Number of GPUs
instance.config.primary.disk.type=Primary disk type:
instance.config.primary.disk.size=Primary disk size:
instance.config.local.ssd=Local SSDs
cluster.info.config.worker.node.desc=Worker nodes:
cluster.info.config.secure.boot=Secure boot:
cluster.info.config.vtpm=VTPM:
cluster.info.config.monitoring=Integrity Monitoring:
cluster.info.config.bucket=Cloud Storage staging bucket:
cluster.info.config.network=네트워크:
cluster.info.internal.ip=Internal IP only:
cluster.info.image.version=Image version:
cluster.info.image.created=Created:
cluster.info.optional.components=Optional components:
cluster.info.properties=프로퍼티:
cluster.info.labels=라벨:
cluster.tab.vb.instances.title=VM Instances
cluster.tab.jobs.title=잡(Job)
cluster.tab.info.title=정보
cluster.tab.applications.title=애플리케이션
job.info.jobId=Job ID:
job.info.jobUuid=Job UUID:
job.info.status=상태:
job.info.status.details=Status details:
job.info.start.date=Start Date:
job.info.elapsed.time=Elapsed time:
job.info.cluster=클러스터:
job.info.type=Job type:
job.info.spark.args=인수:
job.info.labels=라벨
job.info.spark.main.class.or.jar=Main class or jar
job.info.spark.jars=Jars:
job.info.spark.archives=Archives:
job.info.spark.files=Archivo:
job.info.spark.main.pyfile=Main python file:
job.info.spark.main.r.file=Main R file:
job.info.query.file=쿼리:
job.info.query.type=Query source:
job.info.query.file.value=Query file:
job.info.query.text.value=Query text:
job.info.properties=프로퍼티
action.open.cluster.log=Open GC Log Directory
job.info.continue.on.failure=Continue on failure
job.info.client.tags=Client tags
job.hadoop.title=Hadoop
job.spark.title=Spark
job.spark.r.title=SparkR
job.pyspark.title=PySpark
job.hive.title=Hive
job.spark.sql.title=SparkSql
job.pig.title=Pig
job.presto.title=Presto
add.job.title=Submit Job
job.info.max.restart.per.hour=Max restart per hour
job.properties.block.title=프로퍼티
job.label.block.title=라벨
action.add.job.title=Submit Job
job.info.spark.main.class.or.jar.title=Main Class/File Path
job.info.spark.jars.title=JARs
job.info.spark.archives.title=Select Archive
default.gcs.connection.name=GC Dataproc project
job.validation.file.fs={0} must be file with the gs://, hdfs:// or file:// prefix
job.validation.file.archive={0} must be archive type .jar, .tar, .tar.gz, .tgz, .zip.
job.info.spark.archives.hint=Archive files are extracted in the Spark working directory. Can be a GCS file with the gs:// prefix, an HDFS file on the cluster with the hdfs:// prefix, or a local file on the cluster with the file:// prefix. Supported file types: .jar, .tar, .tar.gz, .tgz, .zip.
job.info.spark.jars.hint=Jar files are included in the CLASSPATH. Can be a GCS file with the gs:// prefix, an HDFS file on the cluster with the hdfs:// prefix, or a local file on the cluster with the file:// prefix.
job.info.spark.main.class.or.jar.title.hint=The fully qualified name of a class in a provided or standard jar file, for example, com.example.wordcount, or a provided jar file to use the main class of that jar file
job.info.max.restart.per.hour.hint=Leave blank if you don't want to allow automatic restarts on job failure.
job.info.spark.main.r.file.title=Select Main R File
job.info.spark.additional.r.files.title=Select Additional R File
job.info.spark.additional.r.files=Addtional R files:
job.info.single.file.hint=Can be a GCS file with the gs:// prefix, an HDFS file on the cluster with the hdfs:// prefix, or a local file on the cluster with the file:// prefix
job.info.spark.additional.py.files.title=Select Additional Py File
job.info.spark.additional.py.files=Additional python files:
job.info.spark.main.py.file.title=Select Main Py File
job.query.source.file=파일
job.query.source.text=텍스트
job.query.source.type=Query type:
job.query.file.label=Query file:
job.query.file.dialog.title=Select Query File:
job.query.text.label=Query text:
job.query.text.hint=The query to execute
auth.process.wait.authorization=GCloud Authorization
