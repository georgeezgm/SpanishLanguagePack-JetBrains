action.BigDataTools.CreateClusterConnectionsAction.text=Nueva conexión
action.BigDataTools.Deploy.Configure.text=Crear configuración de envío de Spark...
add.new.arbitrary.cluster.submit.connection.label=Agregar un clúster Spark personalizado...
add.new.ssh.connection.label=Agregar conexión SSH...
app.by.me.value=Creado por el usuario
arbitrary.cluster.wizard.create.connection.title=Clúster Spark personalizado
arbitrary.cluster.wizard.default.radio.button=Utilizar la configuración predeterminada
arbitrary.cluster.wizard.select.sftp.step.desc=Establecer una conexión SFTP con el nodo del controlador
arbitrary.cluster.wizard.select.sftp.step.title=SFTP
arbitrary.cluster.wizard.select.spark.step.desc=Establecer una conexión con el servidor Spark History
arbitrary.cluster.wizard.select.spark.step.title=Spark
arbitrary.cluster.wizard.select.ssh.step.desc=Especifique la configuración SSH para usar Spark-Submit
arbitrary.cluster.wizard.select.ssh.step.title=SSH
arbitrary.cluster.wizard.sftp.custom.radio.button=Configuración de conexión personalizada
arbitrary.cluster.wizard.sftp.none.radio.button=No se requiere conexión SFTP con el nodo del controlador
arbitrary.cluster.wizard.spark.custom.radio.button=Configuración de conexión personalizada
arbitrary.cluster.wizard.spark.none.radio.button=No se requiere conexión al servidor Spark History
arbitrary.cluster.wizard.spark.step.default.uri.comment.text=(localhost en el host del túnel)
artifact.tooltip=Ruta al ejecutable para ejecutar en el clúster
check.connection.availability=Consultar disponibilidad de conexión
check.ssh.connection.ssh.is.not.defined=La configuración SSH no está especificada
cluster.manager.kubernetes=Kubernetes
cluster.manager.local=local
cluster.manager.mesos=Apache Mesos
cluster.manager.nomad=Nomad
cluster.manager.standalone=ser único
cluster.manager.tooltip=Administrador de cluster configurado en el servidor.
cluster.manager.yarn=Hadoop YARN
cluster.status.loading=Cargando...
cluster.status.no.target=sin objetivo
cluster.status.not.selected=<Not Selected>
configuration.description=Configurar envío de Spark
configuration.name=Spark Submit
configuration.name.cluster=grupo
configuration.name.local=Local (en desuso)
configuration.name.python=PySpark
configuration.name.ssh=SSH (obsoleto)
configuration.options.add=Personalización adicional
configuration.options.add.title=Agregar opción de envío
dialog.archives.title=Seleccionar archivo de almacenamiento
dialog.artifactPath.title=Seleccionar aplicación
dialog.driverClassPath.title=Selección de ruta de clase de conductor
dialog.driverLibraryPath.title=Seleccione la ruta de la biblioteca de controladores
dialog.files.title=Seleccione Archivo
dialog.input.spark.command.description=Para completar el formulario Spark Submit, pegue el comando spark-submit aquí
dialog.input.spark.command.label=comando de chispa
dialog.input.spark.command.title=Entrada de chispa
dialog.jars.title=Seleccionar archivo jar
dialog.keytabFile.title=Seleccionar archivo de tabla de claves
dialog.message.adding.other.configurations.not.allowed.here=No se pueden agregar otras configuraciones aquí.
dialog.message.failed.to.create.ssh.process=No se puede generar el proceso SSH
dialog.message.not.found={0} no encontrado
dialog.message.spark.home.should.be.set.to.correct.folder=$SPARK_HOME debe configurarse en la carpeta correcta
dialog.message.specify.application=Por favor especifique la aplicación
dialog.message.ssh.configuration.changed=Ha seleccionado una configuración SSH diferente para el clúster {0}.
dialog.propertiesFile.title=Seleccionar archivo de propiedad
dialog.pyfiles.title=Seleccione el archivo Python
dialog.select.artifact.button.open.artifact.settings=Configuración de artefactos
dialog.select.artifact.empty=No hay artefactos en el proyecto actual.
dialog.select.artifact.link.open.artifact.settings=Configuración de artefactos
dialog.select.artifact.title=Seleccionar artefactos de dependencia
dialog.select.class.empty=Clase no encontrada en la aplicación seleccionada
dialog.select.class.title=nombre de la clase
dialog.sparkHomePath.title=Seleccione el directorio de inicio de Spark
dialog.targetDirectory.title=Seleccionar directorio de destino
dialog.title.select.gradle.artifact.with.task=Seleccionar artefactos y tareas de Gradle
dialog.title.ssh.configuration.changed=La configuración SSH cambió
dialog.workDir.title=Seleccionar directorio de trabajo
edit.ssh.configuration=Editar configuración SSH
error.ssh=Especificar la configuración SSH
error.ssh.config=Se debe especificar la configuración SSH
error.target.config=Seleccionar destino remoto
error.target.config.unresolved=Elija el objetivo remoto adecuado
exportable.SparkSubmitSettings.presentable.name=Herramientas de Big Data Spark Enviar
fun.search.process.text={0} procesamiento
group.ServiceView.AddSparkService.text=Clúster de chispas
inlay.attach.debugger.ssh=Conecte el depurador a través del túnel SSH
label.implicit.cluster.depend.sftp=SFTP\:
label.implicit.cluster.depend.spark.connection=Spark History\:
label.select.ssh.configuration.for.cluster=Seleccione la configuración SSH para el clúster {0}
load.command.string=Cargar comando de envío de chispa
notification.group.sftpsparkfileupload=Carga de archivos SFTP para Spark
open.cluster.info.description=Haga clic para abrir el documento
progress.text.upload.to.host=Sube {0} al alojamiento...
pyspark=PySpark
pyspark.documentation.dataframe.schema=Columna\: {0}
receive.artifact.task=Recibiendo artefacto...
remote.target.arbitrary.cluster.remark=clúster personalizado
remote.target.ssh.remark=SSH
replace.with.allowed.value=Reemplazar con valores permitidos
row.final.command=Comando de envío de resultados
row.final.command.copy=Copiar comando de envío de chispa
row.final.command.hint=Copiar o cargar el comando Spark-Submit completará los campos correspondientes.
services.error.create.spark.connection.canceled.fix.label=reintento de carga
services.error.create.spark.connection.canceled.message=Usuario cancelado
services.error.ssh.config.is.not.found=Configuración SSH no encontrada
services.error.top.cannot.create.spark.connection=No se puede crear una conexión Spark
services.spark.connection.is.not.created=Error al crear la conexión Spark
settings.additional.title=Opciones de envío avanzadas
settings.additional.verbose=Mostrar salida de depuración adicional
settings.application=Solicitud\:
settings.application.arguments=Ejecutar argumentos\:
settings.application.class.hint=--class La clase principal de la aplicación (para aplicaciones Java/Scala).
settings.application.class.name=clase\:
settings.application.class.name.error.msg=Especifique su aplicación primero
settings.application.class.name.error.title=error en el selector de clase
settings.application.hint=Argumentos pasados al método principal de la clase principal (si corresponde).
settings.beforeShellScript=Antes de enviar el guión
settings.beforeShellScript.hint=Script que se ejecutará antes de Spark Submit. Ejemplo\: 'fuente activar py36'
settings.cluster.manager=Administrador de clúster\:
settings.cluster.manager.proxy.user=Usuario proxy\:
settings.cluster.manager.proxy.user.hint=<html>--proxy-user El usuario que se representará al enviar la solicitud.<br>Este argumento no tiene ningún efecto cuando se usa con --principal/--keytab.</html>
settings.cluster.manager.queue=señal\:
settings.cluster.manager.queue.hint=--queue Cola YARN a la que enviar (predeterminado\: 'predeterminado').
settings.cluster.manager.supervise=Activación supervisada
settings.cluster.manager.supervise.hint=Si se especifica --supervise, el controlador se reiniciará si falla.
settings.debug.driver.in.debug.mode=En modo demug
settings.debug.driver.in.run.mode=en modo de ejecución
settings.debug.driver.java.enable=Inicie el controlador Spark sin agente de depuración
settings.debug.driver.java.not.supported=La depuración no se admite en el modo de implementación de clúster
settings.debug.driver.java.port=Puerto de escucha\:
settings.debug.driver.java.port.dynamic=<Dinámico>
settings.debug.driver.java.suspend=Abortar el controlador\:
settings.debug.driver.java.suspend.tooltip=Cuelgue el proceso del controlador Spark hasta que se conecte el depurador
settings.debug.driver.java.tooltip=Agregue '-agentlib\:jdwp' a las opciones de Java del controlador
settings.debug.title=Depuración de chispa
settings.dependencies.files=archivo\:
settings.dependencies.files.hint=--files Una lista de archivos separados por comas que se colocarán en el directorio de trabajo de cada ejecutor. Se puede acceder a la ruta al archivo dentro del ejecutor con SparkFiles.get(fileName).
settings.dependencies.jars=Jar\:
settings.dependencies.jars.hint=--jars Lista separada por comas de archivos jar que se incluirán en la ruta de clases del controlador y del ejecutor.
settings.dependencies.python=archivo py\:
settings.dependencies.python.hint=--py-files Una lista separada por comas de archivos .zip, .egg o .py para agregar al PYTHONPATH de su aplicación Python.
settings.dependencies.title=Dependencia
settings.deploy.mode=Modo de implementación\:
settings.deploy.mode.hint=<html>--deploy-mode Si se debe ejecutar el programa controlador localmente (el 'cliente') o <br>en una de las máquinas de trabajo dentro del clúster (el 'clúster').</html>
settings.deploymode.client=Cliente
settings.deploymode.cluster=grupo
settings.driver.class.path=Ruta de clases del controlador\:
settings.driver.class.path.hint=<html>--driver-class-path Ruta de clase adicional para pasar al controlador.<br>los archivos jar agregados con --jars se incluyen automáticamente en la ruta de clase.</html>
settings.driver.cores=Núcleo del controlador\:
settings.driver.cores.hint=--driver-cores Número de núcleos utilizados por el controlador (solo modo clúster).
settings.driver.java.options=Opciones del controlador Java\:
settings.driver.java.options.hint=--driver-java-options Opciones de Java adicionales para pasar al controlador.
settings.driver.library.path=Ruta de la biblioteca de controladores\:
settings.driver.library.path.hint=--driver-library-path Ruta de biblioteca adicional para pasar al controlador.
settings.driver.memory=Memoria del controlador\:
settings.driver.memory.hint=--driver-memory Memoria del controlador (por ejemplo, 1000M, 2G).
settings.driver.title=conductor
settings.envParams=Variables de entorno
settings.envParams.hint=Variables de entorno adicionales
settings.executor.archives=Archivo\:
settings.executor.archives.hint=<html>--archives Lista de archivos que se extraerán al directorio de trabajo de cada ejecutor.</html>
settings.executor.cores=Núcleo del iniciador\:
settings.executor.cores.hint=<html>--executor-cores Número de núcleos por ejecutor<br>(predeterminado\: 1 en modo YARN o número de todos los núcleos disponibles en el trabajador en modo independiente).</html>
settings.executor.cores.total=Núcleo total del iniciador\:
settings.executor.cores.total.hint=--total-executor-cores Número total de núcleos para todos los ejecutores.
settings.executor.memory=Memoria del Ejecutor\:
settings.executor.memory.default=1G
settings.executor.memory.hint=--executor-memory Memoria por ejecutor (por ejemplo, 1000M, 2G)
settings.executor.number=Número de ejecutores\:
settings.executor.number.hint=<html>--num-executors Número de ejecutores a ejecutar<br>Si la asignación dinámica está habilitada, el número de ejecutores iniciales es el mínimo.</html>
settings.executor.title=lanzacohetes
settings.integration.spark.monitoring=conexión\:
settings.integration.spark.monitoring.add=Agregar nuevo
settings.integration.title=Integración de monitoreo de Spark
settings.isInteractive=interactivo
settings.isInteractive.hint=Ejecute el comando Ejecutar en modo interactivo de shell
settings.kerberos.keytab=Keytab\:
settings.kerberos.keytab.hint=<html>--keytab Ruta completa al archivo que contiene la tabla de claves de la regla especificada anteriormente.<br>Esta tabla de claves se copia en el nodo que ejecuta Application Master a través de Secure Distributed Cache<br>para renovar periódicamente el ticket de inicio de sesión y el token de delegación. </html>
settings.kerberos.principal=regla\:
settings.kerberos.principal.hint=--principal Regla a utilizar al iniciar sesión en el KDC mientras se ejecuta en HDFS seguro.
settings.kerberos.title=Kerberos
settings.master=maestro\:
settings.master.hint=--master spark\://host\:puerto, mesos\://host\:puerto, hilo, k8s\: //https\://host\:puerto o local (predeterminado\: local[*]) .
settings.maven.exclude.packages=Paquete excluido\:
settings.maven.exclude.packages.hint=<html>--exclude-packages Lista de groupId\:artifactIds que se excluirán al resolver dependencias proporcionadas en<br>--packages para evitar conflictos de dependencia.</html>
settings.maven.packages=paquete\:
settings.maven.packages.hint=<html>--packages Lista de coordenadas de Maven para incluir en las rutas de clases del controlador y del ejecutor<br>Buscar en el repositorio local de Maven, luego en el repositorio central de Maven y en los repositorios remotos especificados por --repositories<br>Las coordenadas formateadas como groupId deben ser \:artifactId\:versión.</html>
settings.maven.repositories=Almacenamiento\:
settings.maven.repositories.hint=--repositories Lista de repositorios remotos adicionales para buscar coordenadas de Maven proporcionadas por --packages.
settings.maven.title=Maven
settings.run.target.config=Objetivo remoto\:
settings.run.target.tooltip=Seleccione la aplicación que ejecutará su clúster Spark
settings.shell.title=opciones de shell
settings.shellExecutor=camino del caparazón
settings.shellExecutor.hint=Camino de concha. Se utiliza cuando el modo interactivo está habilitado o antes de configurar el script de envío
settings.spark.config=composición\:
settings.spark.config.hint=--conf Propiedades de configuración de Spark.
settings.spark.home=Inicio de chispa\:
settings.spark.properties.file=Archivo de propiedades\:
settings.spark.properties.file.hint=<html>--properties-file Ruta al archivo desde el cual cargar propiedades adicionales.<br>Si no se especifica, se busca en conf/spark-defaults.conf.</html>
settings.spark.python.sdk=Ejecutar como entorno Python\:
settings.spark.title=Configuración de chispa
settings.ssh.config=Configuración SSH\:
settings.ssh.error.msg=Seleccione su configuración SSH primero
settings.ssh.error.title=error en el selector de archivos
settings.ssh.target.dir=Directorio de carga de destino\:
settings.ssh.target.dir.hint=<html>Ingrese la ruta al directorio en el host remoto donde se cargarán todos los archivos locales.<br>Si los archivos seleccionados ya existen en este directorio, se sobrescribirán.</html>
settings.ssh.title=Opciones SFTP
settings.url.artifact.name=artefactos IDEA
settings.url.artifact.tooltip=artefactos IDEA
settings.url.custom.name=costumbre
settings.url.file.name=archivo
settings.url.gcs.name=almacenamiento GC
settings.url.gcs.tooltip=almacenamiento GC
settings.url.gradle.artifact.name=artefactos gradle
settings.url.gradle.artifact.tooltip=artefactos gradle
settings.url.hdfs.name=HDFS
settings.url.maven.artifact.name=artefactos maven
settings.url.maven.artifact.tooltip=artefactos maven
settings.url.s3.name=S3
settings.url.server.mock.desc=Ruta de archivo\:
settings.url.server.name=archivos del servidor
settings.url.server.tooltip=archivos del servidor
settings.url.upload.name=Subir archivo
settings.url.upload.tooltip=Carga de archivos locales
settings.url.web.name=remoto
settings.workingDirectory=directorio de trabajo
setup.ssh.config=Ajustes de configuración SSH
spark.converter.build.run.configuration.description=La configuración de inicio de 'PySpark' ha cambiado. Necesitará convertir su configuración existente.
spark.submit.gutter.icon.tooltip=Ejecutar en el clúster Spark
sparkhome.tooltip=Directorio de chispa
upload.file.title=Subir archivos de forma remota
upload.files.error=Se produjo una excepción al cargar el archivo. {0}
upload.files.success={0} archivos cargados correctamente
upload.files.through.sftp.to.spark.host=Subir archivos a través de SFTP
upload.target.dir.is.not.found=Directorio de destino ''{0}'' no encontrado
work.directory.tooltip=Apunta a la ubicación de la llamada del script
